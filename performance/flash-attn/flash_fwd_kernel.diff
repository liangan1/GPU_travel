diff --git a/csrc/flash_attn/src/flash_fwd_kernel.h b/csrc/flash_attn/src/flash_fwd_kernel.h
index d492c87..4f33761 100644
--- a/csrc/flash_attn/src/flash_fwd_kernel.h
+++ b/csrc/flash_attn/src/flash_fwd_kernel.h
@@ -24,6 +24,14 @@
 namespace FLASH_NAMESPACE {

 using namespace cute;
+#define SOFTMAX 1
+#define FLASHATTENTION_SOFTMAX_TB_BARRIER 1
+// Optional synchronization around softmax for debugging or instrumentation.
+#ifdef FLASHATTENTION_SOFTMAX_TB_BARRIER
+    #define FLASHATTN_TB_BARRIER() __syncthreads()
+#else
+    #define FLASHATTN_TB_BARRIER() do {} while (0)
+#endif

 ////////////////////////////////////////////////////////////////////////////////////////////////////

@@ -231,7 +239,6 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
     // Repeat the partitioning with identity layouts
     Tensor tQcQ = gmem_thr_copy_QKV.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
     Tensor tKVcKV = gmem_thr_copy_QKV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
-
     // Allocate predicate tensors for k
     Tensor tQpQ = make_tensor<bool>(make_shape(size<2>(tQsQ)));
     Tensor tKVpKV = make_tensor<bool>(make_shape(size<2>(tKsK)));
@@ -337,12 +344,14 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
             // isn't right and we get race conditions.
             cute::cp_async_fence();
         }
-
+        FLASHATTN_TB_BARRIER();
         // TODO: when we have key_padding_mask we'll need to Check_inf
-        masking_step == 0
+        #if SOFTMAX
+       masking_step == 0
             ? softmax.template softmax_rescale_o</*Is_first=*/true,  /*Check_inf=*/Is_causal || Is_local>(acc_s, acc_o, params.scale_softmax_log2)
             : softmax.template softmax_rescale_o</*Is_first=*/false, /*Check_inf=*/Is_causal || Is_local>(acc_s, acc_o, params.scale_softmax_log2);
-
+        #endif
+        FLASHATTN_TB_BARRIER();
         // Convert acc_s from fp32 to fp16/bf16
         Tensor rP = FLASH_NAMESPACE::convert_type<Element>(acc_s);
         int block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
@@ -403,9 +412,11 @@ inline __device__ void compute_attn_1rowblock(const Params &params, const int bi
         mask.template apply_mask</*Causal_mask=*/false>(
             acc_s, n_block * kBlockN, m_block * kBlockM + (tidx / 32) * 16 + (tidx % 32) / 4, kNWarps * 16
         );
-
+        FLASHATTN_TB_BARRIER();
+        #if SOFTMAX
         softmax.template softmax_rescale_o</*Is_first=*/false, /*Check_inf=*/Is_local>(acc_s, acc_o, params.scale_softmax_log2);
-
+        #endif
+        FLASHATTN_TB_BARRIER();
         Tensor rP = FLASH_NAMESPACE::convert_type<Element>(acc_s);
         int block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
         int block_col_idx = n_block * (kBlockN / 32);
